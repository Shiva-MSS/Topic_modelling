{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1ae6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i\\n', 'me\\n', 'my\\n', 'myself\\n', 'we\\n', 'our\\n', 'ours\\n', 'ourselves\\n', 'you\\n', \"you're\\n\", \"you've\\n\", \"you'll\\n\", \"you'd\\n\", 'your\\n', 'yours\\n', 'yourself\\n', 'yourselves\\n', 'he\\n', 'him\\n', 'his\\n', 'himself\\n', 'she\\n', \"she's\\n\", 'her\\n', 'hers\\n', 'herself\\n', 'it\\n', \"it's\\n\", 'its\\n', 'itself\\n', 'they\\n', 'them\\n', 'their\\n', 'theirs\\n', 'themselves\\n', 'what\\n', 'which\\n', 'who\\n', 'whom\\n', 'this\\n', 'that\\n', \"that'll\\n\", 'these\\n', 'those\\n', 'am\\n', 'is\\n', 'are\\n', 'was\\n', 'were\\n', 'be\\n', 'been\\n', 'being\\n', 'have\\n', 'has\\n', 'had\\n', 'having\\n', 'do\\n', 'does\\n', 'did\\n', 'doing\\n', 'a\\n', 'an\\n', 'the\\n', 'and\\n', 'but\\n', 'if\\n', 'or\\n', 'because\\n', 'as\\n', 'until\\n', 'while\\n', 'of\\n', 'at\\n', 'by\\n', 'for\\n', 'with\\n', 'about\\n', 'against\\n', 'between\\n', 'into\\n', 'through\\n', 'during\\n', 'before\\n', 'after\\n', 'above\\n', 'below\\n', 'to\\n', 'from\\n', 'up\\n', 'down\\n', 'in\\n', 'out\\n', 'on\\n', 'off\\n', 'over\\n', 'under\\n', 'again\\n', 'further\\n', 'then\\n', 'once\\n', 'here\\n', 'there\\n', 'when\\n', 'where\\n', 'why\\n', 'how\\n', 'all\\n', 'any\\n', 'both\\n', 'each\\n', 'few\\n', 'more\\n', 'most\\n', 'other\\n', 'some\\n', 'such\\n', 'no\\n', 'nor\\n', 'not\\n', 'only\\n', 'own\\n', 'same\\n', 'so\\n', 'than\\n', 'too\\n', 'very\\n', 's\\n', 't\\n', 'can\\n', 'will\\n', 'just\\n', 'don\\n', \"don't\\n\", 'should\\n', \"should've\\n\", 'now\\n', 'd\\n', 'll\\n', 'm\\n', 'o\\n', 're\\n', 've\\n', 'y\\n', 'ain\\n', 'aren\\n', \"aren't\\n\", 'couldn\\n', \"couldn't\\n\", 'didn\\n', \"didn't\\n\", 'doesn\\n', \"doesn't\\n\", 'hadn\\n', \"hadn't\\n\", 'hasn\\n', \"hasn't\\n\", 'haven\\n', \"haven't\\n\", 'isn\\n', \"isn't\\n\", 'ma\\n', 'mightn\\n', \"mightn't\\n\", 'mustn\\n', \"mustn't\\n\", 'needn\\n', \"needn't\\n\", 'shan\\n', \"shan't\\n\", 'shouldn\\n', \"shouldn't\\n\", 'wasn\\n', \"wasn't\\n\", 'weren\\n', \"weren't\\n\", 'won\\n', \"won't\\n\", 'wouldn\\n', \"wouldn't\\n\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv(\"english.txt\")\n",
    "file = open (\"english.txt\", 'r')\n",
    "\n",
    "for item in file:\n",
    "    stopwords.append(item)\n",
    "print(stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34113ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_1 = '''KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum product prices by the federal government, Geo News reported.Sources said reduction in fares will be applicable on public transport, rickshaw, taxi and other means of traveling. Meanwhile, Karachi Transport Ittehad (KTI) has refused to abide by the government decision.KTI President Irshad Bukhari said the commuters are charged the lowest fares in Karachi as compare to other parts of the country, adding that 80pc vehicles run on Compressed Natural Gas (CNG). Bukhari said Karachi transporters will cut fares when decrease in CNG prices will be made.'''\n",
    "\n",
    "article_2 = '''HONG KONG:  Hong Kong shares opened 0.66 percent lower Monday following a tepid lead from Wall Street, as the first full week of the new year kicked off. The benchmark Hang Seng Index dipped 158.63 points to 23,699.19.'''\n",
    "\n",
    "article_3 = '''KARACHI: Wholesale market rates for sugar dropped to less than Rs 50 per kg following the resumption of sugar cane crushing by sugar mills in Sindh. Within two days, the rate dropped by Rs 1.70 to Rs 49.80 per kg in Karachi Whole Sale Market. According to dealers, the resumption of sugar cane crushing by the mills stabilised the supply to the market with an immediate effect on price as well. Industry experts said that the quality of sugar cane is excellent in Sindh and approximately 100 kg of sugar cane can produce 11 kg of sugar.'''\n",
    "\n",
    "article_4 = '''ISLAMABAD: Long queues of vehicles on fuel stations were visible in different parts of the country as the petrol became rare commodity on Thursday. Federal Minister for Petroleum Shahid Khaqan Abbasi says \"it may take up to ten days to bring the situation to normality\". He claimed that northern areas of Pakistan had been facing the petrol shortage. The minister cited the recent decline in petroleum prices and delay in a shipment as reasons for the shortage.He said situation would improve as soon as shipment reached Pakistan. Sources told Geo News hat due to financial restraints the Pakistan State Oil has been unable import petrol.'''\n",
    "\n",
    "article_5 = '''KARACHI: The final shipment of Chinese manufactured Rail Engines arrived in Pakistan on Friday. Federal Railways Minister, Khwaja Saad Rafique says, the inclusion of the new engines will help ease the shortfall faced by Pakistan Railways. The shipment includes 2000 and 3000-horse-power engines which will be used to pull freight bogeys. Rafique told journalists, the inclusion of 15 new engines has brought Pakistan Railways total strength to 268 engines however more engines are still required.'''\n",
    "\n",
    "article_6 = '''SYDNEY: Cricket fever has gripped Australia with the World Cup just days away. Fans from around the world have thronged to the country and hotels are capitalising. Prices of rooms have almost doubled to 300 dollars and hotels are experiencing full bookings. Experts estimate that during the mega event Australia will generate 1.5 million US dollars just from hotel bookings. If the cost of internal air travel, taxis and tickets is taken into consideration, Australia stands to generate two million US dollars during the World Cup.'''\n",
    "\n",
    "article_7 = '''SAN FRANCISCO: Apple Inc aims to begin producing electric vehicles as early as 2020, Bloomberg reported. The report cited people with knowledge of the matter as saying, a seemingly aggressive target for a mobile devices maker with little experience in car manufacture.The iPhone maker is pushing its \"car team\" of about 200 people to meet that goal. But Apple may decide to scrap its car-making effort, or delay it, if executives grew unhappy with its progress, the news agency said.'''\n",
    "\n",
    "article_8 = '''LAHORE: Federal Minister for Railways, Khawaja Saad Rafique Tuesday announced good news of pay-raise for the employees of Pakistan Railways. In a media statement, the Minister disclosed that a summary for increase in salaries for the employees of Pakistan Railways has been forwarded to the Prime Minister. He also said that the government had also chalked out a plan to build houses for the Railways workers. Khawaja Saad Rafique said it was expected that the salaries of Railway Police may witness a jump of 20 percent. He also announced the government\\x92s plan to launch a new train service between Karachi and Islamabad.'''\n",
    "\n",
    "article_9 = '''ISLAMABAD: The Federal Cabinet on Tuesday approved the budget strategy paper, sources revealed to Geo News. During the cabinet meeting, Prime Minister Nawaz Sharif said tax rate had to be reduced to increase revenue. He added that people would happily pay taxes if the rate was reduced. The prime minister directed the cabinet to provide maximum relief to people in the budget, emphasising that the economic impact should reach people.'''\n",
    "\n",
    "article_10 = '''BEIJING: China will keep the yuan basically stable against a basket of currencies and there is no basis for continued yuan depreciation, central bank vice governor Yi Gang said on Sunday. China also will keep foreign exchange reserves at appropriate levels, Yi said.'''\n",
    "\n",
    "articles = [article_1, article_2, article_3, article_4, article_5, article_6, article_7, article_8, article_9, article_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16e43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized if not re.match(r'\\d+',token)])\n",
    "  return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e4d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Are the tf-idf scores the same?\n",
      "0                             YES\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi          0          0          0          1          0          0   \n",
      "abide           1          0          0          0          0          0   \n",
      "about           0          0          0          0          0          0   \n",
      "accord          0          0          1          0          0          0   \n",
      "add             1          0          0          0          0          0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world           0          0          0          0          0          3   \n",
      "would           0          0          0          1          0          0   \n",
      "year            0          1          0          0          0          0   \n",
      "yi              0          0          0          0          0          0   \n",
      "yuan            0          0          0          0          0          0   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi          0          0          0           0  \n",
      "abide           0          0          0           0  \n",
      "about           1          0          0           0  \n",
      "accord          0          0          0           0  \n",
      "add             0          0          1           0  \n",
      "...           ...        ...        ...         ...  \n",
      "world           0          0          0           0  \n",
      "would           0          0          1           0  \n",
      "year            0          0          0           0  \n",
      "yi              0          0          0           2  \n",
      "yuan            0          0          0           2  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
      "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
      "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
      "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
      "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    2.704748        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   2.299283    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   2.299283    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    5.409496  \n",
      "yuan     0.000000        0.0   0.000000    5.409496  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
      "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
      "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
      "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
      "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    2.704748        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   2.299283    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   2.299283    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    5.409496  \n",
      "yuan     0.000000        0.0   0.000000    5.409496  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "Article 1    fare\n",
      "dtype: object\n",
      "Article 2    hong\n",
      "dtype: object\n",
      "Article 3    sugar\n",
      "dtype: object\n",
      "Article 4    petrol\n",
      "dtype: object\n",
      "Article 5    engine\n",
      "dtype: object\n",
      "Article 6    australia\n",
      "dtype: object\n",
      "Article 7    car\n",
      "dtype: object\n",
      "Article 8    railway\n",
      "dtype: object\n",
      "Article 9    cabinet\n",
      "dtype: object\n",
      "Article 10    china\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva shankar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from articles import articles\n",
    "\n",
    "# import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# view article\n",
    "#print(articles[5])\n",
    "\n",
    "\n",
    "# preprocess articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "#print(processed_articles[5])\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "\n",
    "# convert counts to tf-idf\n",
    "transformer = TfidfTransformer(norm = None)\n",
    "\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles )\n",
    "\n",
    "# check if tf-idf scores are equal\n",
    "\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
    "else:\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
    "\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_word_counts)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get highest scoring tf-idf term for each article\n",
    "for i in range(1, 11):\n",
    "  print(df_tf_idf[[f'Article {i}']].idxmax())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2526249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva shankar\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
